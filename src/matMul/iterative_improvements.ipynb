{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92901a84",
   "metadata": {},
   "source": [
    "## Iteratively Improving MatMul Kernel"
   ]
  },
  {
   "cell_type": "code",
   "id": "ffdc039b",
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport numpy as np\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\nimport os\nimport time"
  },
  {
   "cell_type": "markdown",
   "id": "114a359a",
   "metadata": {},
   "source": "### Inline CUDA Kernel System",
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "148f0e83",
   "metadata": {},
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\nimport pandas as pd\nfrom statistics import mean, stdev\n\nclass CudaKernelTester:\n    def __init__(self):\n        self.compiled_kernels = {}\n        self.benchmark_results = {}\n    \n    def compile_kernel(self, kernel_name, cuda_code, cpp_code):\n        \"\"\"Compile inline CUDA kernel with C++ wrapper\"\"\"\n        try:\n            module = load_inline(\n                name=kernel_name,\n                cpp_sources=[cpp_code],\n                cuda_sources=[cuda_code],\n                verbose=True,\n                with_cuda=True\n            )\n            self.compiled_kernels[kernel_name] = module\n            print(f\"âœ“ Successfully compiled kernel: {kernel_name}\")\n            return module\n        except Exception as e:\n            print(f\"âœ— Failed to compile kernel {kernel_name}: {str(e)}\")\n            return None\n    \n    def benchmark_kernel(self, kernel_name, A, B, C, num_runs=10, warmup_runs=3):\n        \"\"\"Benchmark a kernel with multiple runs and return statistics\"\"\"\n        if kernel_name not in self.compiled_kernels:\n            return None\n        \n        module = self.compiled_kernels[kernel_name]\n        \n        # Warmup runs\n        for _ in range(warmup_runs):\n            module.matmul_forward(A, B, C)\n            torch.cuda.synchronize()\n        \n        # Benchmark runs\n        times = []\n        for _ in range(num_runs):\n            torch.cuda.synchronize()\n            start_event = torch.cuda.Event(enable_timing=True)\n            end_event = torch.cuda.Event(enable_timing=True)\n            \n            start_event.record()\n            module.matmul_forward(A, B, C)\n            end_event.record()\n            \n            torch.cuda.synchronize()\n            times.append(start_event.elapsed_time(end_event))  # ms\n        \n        return {\n            'mean_ms': mean(times),\n            'std_ms': stdev(times) if len(times) > 1 else 0,\n            'min_ms': min(times),\n            'max_ms': max(times),\n            'all_times': times\n        }\n    \n    def benchmark_pytorch(self, A, B, num_runs=10, warmup_runs=3):\n        \"\"\"Benchmark PyTorch matmul with multiple runs\"\"\"\n        # Warmup runs\n        for _ in range(warmup_runs):\n            torch.mm(A, B)\n            torch.cuda.synchronize()\n        \n        # Benchmark runs\n        times = []\n        for _ in range(num_runs):\n            torch.cuda.synchronize()\n            start_event = torch.cuda.Event(enable_timing=True)\n            end_event = torch.cuda.Event(enable_timing=True)\n            \n            start_event.record()\n            result = torch.mm(A, B)\n            end_event.record()\n            \n            torch.cuda.synchronize()\n            times.append(start_event.elapsed_time(end_event))  # ms\n        \n        return {\n            'mean_ms': mean(times),\n            'std_ms': stdev(times) if len(times) > 1 else 0,\n            'min_ms': min(times),\n            'max_ms': max(times),\n            'all_times': times,\n            'result': result\n        }\n    \n    def calculate_performance_metrics(self, m, k, n, time_ms):\n        \"\"\"Calculate FLOPS and memory bandwidth metrics\"\"\"\n        # Matrix multiplication FLOPS: 2*M*N*K (multiply + add for each element)\n        flops = 2 * m * n * k\n        gflops = flops / (time_ms * 1e-3) / 1e9  # GFLOPS\n        \n        # Memory bandwidth calculation\n        # Read A (M*K), Read B (K*N), Write C (M*N) - all float32 (4 bytes)\n        bytes_transferred = (m * k + k * n + m * n) * 4\n        bandwidth_gb_s = bytes_transferred / (time_ms * 1e-3) / 1e9  # GB/s\n        \n        return {\n            'gflops': gflops,\n            'bandwidth_gb_s': bandwidth_gb_s,\n            'arithmetic_intensity': flops / bytes_transferred  # FLOPS per byte\n        }\n    \n    def test_matmul_kernel(self, kernel_name, test_sizes=[(2,3,2), (32,64,32), (128,128,128), (512,512,512)], num_runs=10):\n        \"\"\"Test matrix multiplication kernel with comprehensive performance analysis\"\"\"\n        if kernel_name not in self.compiled_kernels:\n            print(f\"âœ— Kernel {kernel_name} not found\")\n            return False\n        \n        all_passed = True\n        results = []\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"BENCHMARKING KERNEL: {kernel_name}\")\n        print(f\"{'='*60}\")\n        \n        for m, k, n in test_sizes:\n            print(f\"\\nTesting size [{m}Ã—{k}] Ã— [{k}Ã—{n}]\")\n            print(\"-\" * 40)\n            \n            # Create test matrices\n            A = torch.randn(m, k, device='cuda', dtype=torch.float32).contiguous()\n            B = torch.randn(k, n, device='cuda', dtype=torch.float32).contiguous()\n            C = torch.zeros(m, n, device='cuda', dtype=torch.float32).contiguous()\n            \n            # Benchmark custom kernel\n            custom_stats = self.benchmark_kernel(kernel_name, A, B, C, num_runs)\n            if custom_stats is None:\n                continue\n            \n            # Benchmark PyTorch\n            pytorch_stats = self.benchmark_pytorch(A, B, num_runs)\n            \n            # Check correctness\n            correctness_passed = torch.allclose(C, pytorch_stats['result'], atol=1e-4)\n            \n            # Calculate performance metrics\n            custom_metrics = self.calculate_performance_metrics(m, k, n, custom_stats['mean_ms'])\n            pytorch_metrics = self.calculate_performance_metrics(m, k, n, pytorch_stats['mean_ms'])\n            \n            # Print results\n            print(f\"Correctness: {'âœ“ PASSED' if correctness_passed else 'âœ— FAILED'}\")\n            if not correctness_passed:\n                max_diff = torch.max(torch.abs(C - pytorch_stats['result']))\n                print(f\"Max difference: {max_diff:.6f}\")\n                all_passed = False\n            \n            print(f\"\\nðŸ“Š PERFORMANCE COMPARISON:\")\n            print(f\"{'Metric':<20} {'Custom':<15} {'PyTorch':<15} {'Speedup':<10}\")\n            print(\"-\" * 65)\n            \n            # Timing comparison\n            speedup = pytorch_stats['mean_ms'] / custom_stats['mean_ms']\n            print(f\"{'Time (ms)':<20} {custom_stats['mean_ms']:<15.3f} {pytorch_stats['mean_ms']:<15.3f} {speedup:<10.2f}x\")\n            print(f\"{'Â±Std (ms)':<20} {custom_stats['std_ms']:<15.3f} {pytorch_stats['std_ms']:<15.3f}\")\n            \n            # Performance metrics\n            gflops_speedup = custom_metrics['gflops'] / pytorch_metrics['gflops']\n            bandwidth_speedup = custom_metrics['bandwidth_gb_s'] / pytorch_metrics['bandwidth_gb_s']\n            \n            print(f\"{'GFLOPS':<20} {custom_metrics['gflops']:<15.1f} {pytorch_metrics['gflops']:<15.1f} {gflops_speedup:<10.2f}x\")\n            print(f\"{'Bandwidth (GB/s)':<20} {custom_metrics['bandwidth_gb_s']:<15.1f} {pytorch_metrics['bandwidth_gb_s']:<15.1f} {bandwidth_speedup:<10.2f}x\")\n            print(f\"{'Arith. Intensity':<20} {custom_metrics['arithmetic_intensity']:<15.2f} {pytorch_metrics['arithmetic_intensity']:<15.2f}\")\n            \n            # Store results for analysis\n            result_entry = {\n                'kernel': kernel_name,\n                'size': f\"{m}Ã—{k}Ã—{n}\",\n                'm': m, 'k': k, 'n': n,\n                'custom_time_ms': custom_stats['mean_ms'],\n                'pytorch_time_ms': pytorch_stats['mean_ms'],\n                'speedup': speedup,\n                'custom_gflops': custom_metrics['gflops'],\n                'pytorch_gflops': pytorch_metrics['gflops'],\n                'custom_bandwidth': custom_metrics['bandwidth_gb_s'],\n                'pytorch_bandwidth': pytorch_metrics['bandwidth_gb_s'],\n                'correctness': correctness_passed\n            }\n            results.append(result_entry)\n        \n        # Store results for later analysis\n        self.benchmark_results[kernel_name] = results\n        \n        return all_passed\n    \n    def compare_kernels(self, kernel_names, test_sizes=[(128,128,128), (512,512,512), (1024,1024,1024)]):\n        \"\"\"Compare multiple kernels across different sizes\"\"\"\n        print(f\"\\n{'='*80}\")\n        print(f\"KERNEL COMPARISON\")\n        print(f\"{'='*80}\")\n        \n        comparison_data = []\n        \n        for size in test_sizes:\n            m, k, n = size\n            print(f\"\\nSize: [{m}Ã—{k}] Ã— [{k}Ã—{n}]\")\n            print(\"-\" * 50)\n            \n            A = torch.randn(m, k, device='cuda', dtype=torch.float32).contiguous()\n            B = torch.randn(k, n, device='cuda', dtype=torch.float32).contiguous()\n            \n            # Benchmark PyTorch as baseline\n            pytorch_stats = self.benchmark_pytorch(A, B, num_runs=10)\n            pytorch_metrics = self.calculate_performance_metrics(m, k, n, pytorch_stats['mean_ms'])\n            \n            size_results = {'size': f\"{m}Ã—{k}Ã—{n}\", 'PyTorch': pytorch_stats['mean_ms']}\n            \n            print(f\"{'Kernel':<20} {'Time (ms)':<12} {'GFLOPS':<10} {'Speedup':<10}\")\n            print(\"-\" * 55)\n            print(f\"{'PyTorch (baseline)':<20} {pytorch_stats['mean_ms']:<12.3f} {pytorch_metrics['gflops']:<10.1f} {'1.00x':<10}\")\n            \n            for kernel_name in kernel_names:\n                if kernel_name in self.compiled_kernels:\n                    C = torch.zeros(m, n, device='cuda', dtype=torch.float32).contiguous()\n                    custom_stats = self.benchmark_kernel(kernel_name, A, B, C, num_runs=10)\n                    if custom_stats:\n                        custom_metrics = self.calculate_performance_metrics(m, k, n, custom_stats['mean_ms'])\n                        speedup = pytorch_stats['mean_ms'] / custom_stats['mean_ms']\n                        \n                        print(f\"{kernel_name:<20} {custom_stats['mean_ms']:<12.3f} {custom_metrics['gflops']:<10.1f} {speedup:<10.2f}x\")\n                        size_results[kernel_name] = custom_stats['mean_ms']\n            \n            comparison_data.append(size_results)\n        \n        return comparison_data\n    \n    def plot_performance_comparison(self, kernel_names=None):\n        \"\"\"Create performance comparison charts\"\"\"\n        if not self.benchmark_results:\n            print(\"No benchmark results available. Run tests first.\")\n            return\n        \n        kernels_to_plot = kernel_names or list(self.benchmark_results.keys())\n        \n        # Prepare data for plotting\n        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n        fig.suptitle('Kernel Performance Comparison', fontsize=16)\n        \n        # Collect all data\n        all_data = []\n        for kernel in kernels_to_plot:\n            if kernel in self.benchmark_results:\n                for result in self.benchmark_results[kernel]:\n                    all_data.append({**result, 'total_elements': result['m'] * result['k'] * result['n']})\n        \n        if not all_data:\n            print(\"No data to plot\")\n            return\n        \n        df = pd.DataFrame(all_data)\n        \n        # Plot 1: Speedup vs Matrix Size\n        ax1 = axes[0, 0]\n        for kernel in kernels_to_plot:\n            kernel_data = df[df['kernel'] == kernel]\n            if not kernel_data.empty:\n                ax1.plot(kernel_data['total_elements'], kernel_data['speedup'], \n                        marker='o', label=kernel, linewidth=2)\n        ax1.set_xlabel('Total Elements (MÃ—KÃ—N)')\n        ax1.set_ylabel('Speedup vs PyTorch')\n        ax1.set_title('Speedup vs Matrix Size')\n        ax1.set_xscale('log')\n        ax1.grid(True, alpha=0.3)\n        ax1.legend()\n        ax1.axhline(y=1, color='red', linestyle='--', alpha=0.5, label='PyTorch baseline')\n        \n        # Plot 2: GFLOPS Comparison\n        ax2 = axes[0, 1]\n        for kernel in kernels_to_plot:\n            kernel_data = df[df['kernel'] == kernel]\n            if not kernel_data.empty:\n                ax2.plot(kernel_data['total_elements'], kernel_data['custom_gflops'], \n                        marker='s', label=kernel, linewidth=2)\n        ax2.set_xlabel('Total Elements (MÃ—KÃ—N)')\n        ax2.set_ylabel('GFLOPS')\n        ax2.set_title('Computational Performance')\n        ax2.set_xscale('log')\n        ax2.grid(True, alpha=0.3)\n        ax2.legend()\n        \n        # Plot 3: Memory Bandwidth\n        ax3 = axes[1, 0]\n        for kernel in kernels_to_plot:\n            kernel_data = df[df['kernel'] == kernel]\n            if not kernel_data.empty:\n                ax3.plot(kernel_data['total_elements'], kernel_data['custom_bandwidth'], \n                        marker='^', label=kernel, linewidth=2)\n        ax3.set_xlabel('Total Elements (MÃ—KÃ—N)')\n        ax3.set_ylabel('Memory Bandwidth (GB/s)')\n        ax3.set_title('Memory Bandwidth Utilization')\n        ax3.set_xscale('log')\n        ax3.grid(True, alpha=0.3)\n        ax3.legend()\n        \n        # Plot 4: Execution Time Comparison\n        ax4 = axes[1, 1]\n        for kernel in kernels_to_plot:\n            kernel_data = df[df['kernel'] == kernel]\n            if not kernel_data.empty:\n                ax4.plot(kernel_data['total_elements'], kernel_data['custom_time_ms'], \n                        marker='d', label=kernel, linewidth=2)\n        ax4.set_xlabel('Total Elements (MÃ—KÃ—N)')\n        ax4.set_ylabel('Execution Time (ms)')\n        ax4.set_title('Execution Time vs Matrix Size')\n        ax4.set_xscale('log')\n        ax4.set_yscale('log')\n        ax4.grid(True, alpha=0.3)\n        ax4.legend()\n        \n        plt.tight_layout()\n        plt.show()\n\n# Initialize the enhanced tester\nkernel_tester = CudaKernelTester()"
  },
  {
   "cell_type": "code",
   "id": "4f36c935",
   "metadata": {},
   "outputs": [],
   "source": "# Example: Basic Matrix Multiplication Kernel\n\ncuda_kernel_v1 = \"\"\"\n__global__ void matmul_kernel(float* A, float* B, float* C, int M, int K, int N) {\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (row < M && col < N) {\n        float sum = 0.0f;\n        for (int k = 0; k < K; k++) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n\"\"\"\n\ncpp_wrapper_v1 = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\nvoid matmul_forward(torch::Tensor A, torch::Tensor B, torch::Tensor C) {\n    const int M = A.size(0);\n    const int K = A.size(1);\n    const int N = B.size(1);\n    \n    const dim3 blockSize(16, 16);\n    const dim3 gridSize((N + blockSize.x - 1) / blockSize.x, \n                       (M + blockSize.y - 1) / blockSize.y);\n    \n    matmul_kernel<<<gridSize, blockSize>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K, N\n    );\n    \n    cudaDeviceSynchronize();\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"matmul_forward\", &matmul_forward, \"Matrix multiplication forward\");\n}\n\"\"\"\n\n# Compile and test the basic kernel\nkernel_tester.compile_kernel(\"matmul_v1\", cuda_kernel_v1, cpp_wrapper_v1)"
  },
  {
   "cell_type": "code",
   "id": "sl9k3yttkd",
   "source": "# Test the basic kernel\nkernel_tester.test_matmul_kernel(\"matmul_v1\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "w8gkxspj3d",
   "source": "### Optimized Version with Shared Memory",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}